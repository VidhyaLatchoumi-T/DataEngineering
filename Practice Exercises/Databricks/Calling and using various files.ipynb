{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7ccdf4-378b-443e-a39d-2299365ea69d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_data.csv\", \"dbfs:/FileStore/employee_data.csv\")\n",
    "#Load CSV data into a DataFrame\n",
    "df_employee = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/employee_data.csv\")\n",
    "#Write DataFrame to Delta format\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9deca7-87e1-4b12-a624-94325a06ba31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "# Define schema for JSON file\n",
    "schema=StructType([ \n",
    "                   StructField(\"ProductID\", IntegerType(), True), \n",
    "                   StructField(\"ProductName\", StringType(), True), \n",
    "                   StructField(\"Category\", StringType(), True), \n",
    "                   StructField(\"Price\", DoubleType(), True),\n",
    "                   StructField(\"Stock\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a9bf1b-803e-4d27-8813-004d04c4aa2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the file from Workspace to DBFS CELL 2\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/product_data.json\", \"dbfs:/FileStore/product_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e081a75-f5a0-4c9a-976d-03e97e57ac04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------+-----+\n|ProductID|ProductName|   Category| Price|Stock|\n+---------+-----------+-----------+------+-----+\n|      101|     Laptop|Electronics|1200.0|   35|\n|      102| Smartphone|Electronics| 800.0|   80|\n|      103| Desk Chair|  Furniture| 150.0|   60|\n|      104|    Monitor|Electronics| 300.0|   45|\n|      105|       Desk|  Furniture| 350.0|   25|\n+---------+-----------+-----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Load JSON data with schema --CELL 3\n",
    "df_product=spark.read.format(\"json\").schema(schema).load(\"dbfs:/FileStore/product_data.json\") \n",
    "df_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c8a094-4f47-4678-924b-7b50bcefbddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temp view for SQL operations CELL 4\n",
    "df_product.createOrReplaceTempView(\"product_view\")\n",
    "#Create a Delta table from the view \n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE delta_product_table_new\n",
    "USING DELTA\n",
    "AS SELECT * FROM product_view\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cd12af-e37a-40aa-9c27-cd7ceff176fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_updates.csv\", \"dbfs:/FileStore/employee_updates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5641929-5830-4652-87fd-2182574d70e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert employee CSV data to Delta format\n",
    "df_employee=spark.read.format(\"csv\").option (\"header\", \"true\").load(\"/FileStore/employee_data.csv\")\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_data\")\n",
    "#Convert employee updates CSV data to Delta format\n",
    "df_employee_updates= spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee_updates.csv\")\n",
    "df_employee_updates.write.format(\"delta\").mode (\"overwrite\").save(\"/delta/employee_updates\")\n",
    "#Load Delta tables\n",
    "df_employee=spark.read.format(\"delta\").load(\"/delta/employee_data\")\n",
    "df_employee_updates= spark.read.format(\"delta\").load(\"/delta/employee_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb71d473-a661-4645-8fdc-da2a3740ce11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create temporary views for SQL operations\n",
    "df_employee.createOrReplaceTempView(\"delta_employee\") \n",
    "df_employee_updates.createOrReplaceTempView(\"employee_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45fa17bf-fd0e-43b2-affb-f13b3484b2f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_employee AS target \n",
    "          USING employee_updates AS source \n",
    "          ON target.EmployeeID = source.EmployeeID \n",
    "          WHEN MATCHED THEN UPDATE SET target.Salary = source.Salary,target.Department = source.Department \n",
    "          WHEN NOT MATCHED THEN INSERT (EmployeeID, Name, Department, JoiningDate, Salary)\n",
    "          VALUES (source.EmployeeID, source.Name, source.Department, source.JoiningDate, source.Salary)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9774db80-d349-4796-9a55-0199ee1a7f56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n|      1001|     John Doe|        HR| 2021-01-15| 58000|\n|      1009|  Sarah Adams| Marketing| 2021-09-01| 60000|\n|      1010|  Robert King|        IT| 2022-01-10| 62000|\n+----------+-------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Query the Delta table to check if the data was updated or inserted correctly \n",
    "spark.sql(\"SELECT * FROM delta_employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20da3ab9-5d4d-41ff-8e0c-0aa66a6f9e7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write the employee dataframe to a delta table\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a273aa9-c23d-44d7-a2aa-3c0661a9da4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Register the delta table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_employee_table USING DELTA LOCATION '/delta/employee_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3c8b4b-4d2e-491c-81f6-8cb068572edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimise the delta table\n",
    "spark.sql(\"OPTIMIZE delta_employee_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53921ac1-3ecb-4fe8-a14c-f1a61e1469e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                                |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|6      |2024-09-13 04:22:57|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                       |NULL|{2996383389176158}|0912-053226-p5usobai|5          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 1662}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|5      |2024-09-13 04:13:22|2929584751483774|azuser2141_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                                     |NULL|{2996383389176158}|0912-053226-p5usobai|4          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 2899, p25FileSize -> 1662, numDeletionVectorsRemoved -> 1, minFileSize -> 1662, numAddedFiles -> 1, maxFileSize -> 1662, p75FileSize -> 1662, p50FileSize -> 1662, numAddedBytes -> 1662}                                                                                                                                                                                                                                                                                                                                                                                                                                                   |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|4      |2024-09-13 04:13:18|2929584751483774|azuser2141_mml.local@techademy.com|MERGE    |{predicate -> [\"(EmployeeID#2482 = EmployeeID#2492)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{2996383389176158}|0912-053226-p5usobai|3          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1342, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 6183, materializeSourceTimeMs -> 1064, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 3086, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1944}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|3      |2024-09-13 04:13:03|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                       |NULL|{2996383389176158}|0912-053226-p5usobai|2          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1557}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2024-09-13 04:04:59|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                       |NULL|{2996383389176158}|0912-053226-p5usobai|1          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1557}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2024-09-12 11:52:59|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                       |NULL|{2996383389176158}|0912-053226-p5usobai|0          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1557}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2024-09-12 11:52:49|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                       |NULL|{2996383389176158}|0912-053226-p5usobai|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1557}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Describe history\n",
    "spark.sql(\"DESCRIBE HISTORY delta_employee_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e496ca7a-0998-49e7-aad3-05728ea13e25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_employee_table ZORDER BY Department\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tVACUUM delta_employee_table RETAIN 168 HOURS\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-09-12 15:27:56",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
