{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMvuZO8Lr6XCoqGW6o24ut"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxdkVkoBOkuc","executionInfo":{"status":"ok","timestamp":1727067362372,"user_tz":-330,"elapsed":53339,"user":{"displayName":"VIDHYA LATCHOUMI T","userId":"15527498621438682456"}},"outputId":"a0c8627d-9807-4583-f7aa-c30050517405"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=b66e51ce539100174cd7b575d293206b176bb8977d748cc6e040dccbc05fd0b0\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.2\n"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","source":["### Exercise 1: Creating a Complete ETL Pipeline using Delta Live Tables(DLT)"],"metadata":{"id":"eItxSVLxVeCD"}},{"cell_type":"markdown","source":["1. Create Delta Live Table (DLT) Pipeline:"],"metadata":{"id":"KdGERQ3NSExg"}},{"cell_type":"markdown","source":["Step 1: Ingest Raw Data from CSV Files"],"metadata":{"id":"-XxM-aMCPrMc"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"tReqt79vRVzh"}},{"cell_type":"code","source":["import dlt\n","\n","@dlt.table\n","def raw_transactions():\n","    \"\"\"Ingest raw data from the CSV file.\"\"\"\n","    return spark.read.csv(\"/dbfs/path/to/transactions.csv\", header=True, inferSchema=True)\n"],"metadata":{"id":"cMnuqpD2Q-w6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sql"],"metadata":{"id":"CBGe5HZ3RZv0"}},{"cell_type":"code","source":["CREATE OR REFRESH LIVE TABLE raw_transactions AS\n","SELECT *\n","FROM read_csv(\"/dbfs/path/to/transactions.csv\");"],"metadata":{"id":"hai3I0rLRaua"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 2: Apply Transformations"],"metadata":{"id":"HfGr4VwXRhYr"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"V7ZieleWRhYs"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","@dlt.table\n","def transformed_transactions():\n","    \"\"\"Transform data by calculating the TotalAmount.\"\"\"\n","    return (\n","        dlt.read(\"raw_transactions\")\n","        .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n","    )\n"],"metadata":{"id":"9WOgAyxwRhYs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sql"],"metadata":{"id":"VCTBpLRdRhYt"}},{"cell_type":"code","source":["CREATE OR REFRESH LIVE TABLE transformed_transactions AS\n","SELECT\n","    TransactionID,\n","    TransactionDate,\n","    CustomerID,\n","    Product,\n","    Quantity,\n","    Price,\n","    Quantity * Price AS TotalAmount\n","FROM\n","    LIVE.raw_transactions;"],"metadata":{"id":"QlfyyRmoRhYt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 3: Write the Final Data into a Delta Table"],"metadata":{"id":"AM5dg50VR4k_"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"fxyt-ka_R4lA"}},{"cell_type":"code","source":["@dlt.table\n","def final_transactions():\n","    \"\"\"Write the final data into a Delta table.\"\"\"\n","    return dlt.read(\"transformed_transactions\")\n"],"metadata":{"id":"CIDQZYuSR4lA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sql"],"metadata":{"id":"7HWJWr-qR4lB"}},{"cell_type":"code","source":["CREATE OR REFRESH LIVE TABLE final_transactions AS\n","SELECT *\n","FROM LIVE.transformed_transactions;"],"metadata":{"id":"Os19siGJR4lB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Write DLT in Python"],"metadata":{"id":"S0JUJsR6Vvl4"}},{"cell_type":"code","source":["import dlt\n","from pyspark.sql.functions import col\n","\n","@dlt.table\n","def raw_transactions():\n","    # Step 1: Read data from the CSV file\n","    return spark.read.csv(\"/content/sample_data/transactions.csv\", header=True, inferSchema=True)\n","\n","@dlt.table\n","def transformed_transactions():\n","    # Step 2: Apply transformations to calculate total transaction amount\n","    return (\n","        dlt.read(\"raw_transactions\")\n","        .select(\n","            col(\"TransactionID\"),\n","            col(\"TransactionDate\"),\n","            col(\"CustomerID\"),\n","            col(\"Product\"),\n","            col(\"Quantity\"),\n","            col(\"Price\"),\n","            (col(\"Quantity\") * col(\"Price\")).alias(\"TotalAmount\")  # Calculate total amount\n","        )\n","    )\n"],"metadata":{"id":"pb5QiRVbV2aO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Write DLT in SQL"],"metadata":{"id":"aOY1DkCoV5jl"}},{"cell_type":"code","source":["-- Step 1: Create Raw Transactions Table\n","CREATE OR REFRESH LIVE TABLE raw_transactions AS\n","SELECT *\n","FROM read_csv('/content/sample_data/transactions.csv', header = true);\n","\n","-- Step 2: Create Transformed Transactions Table\n","CREATE OR REFRESH LIVE TABLE transformed_transactions AS\n","SELECT\n","    TransactionID,\n","    TransactionDate,\n","    CustomerID,\n","    Product,\n","    Quantity,\n","    Price,\n","    Quantity * Price AS TotalAmount  -- Calculate total amount\n","FROM\n","    LIVE.raw_transactions;\n"],"metadata":{"id":"--JiQ58_V9Uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Monitor the Pipeline"],"metadata":{"id":"45qmcmLOWMDl"}},{"cell_type":"markdown","source":["1. Access the DLT UI:\n","\n","  Open the Databricks workspace.\n","\n","  Click on \"Workflows\" in the sidebar.\n","  \n","  Select \"Delta Live Tables.\"\n","\n","2. View the Pipeline:\n","\n","  A list of DLT pipelines will be displayed. Click on the desired pipeline to view its details.\n","  \n","  The status of each table can be checked, along with any errors or warnings.\n","\n","3. Examine Execution Details:\n","\n","  Execution history, logs, and performance metrics for each step in the pipeline can be reviewed."],"metadata":{"id":"HaFj2KvCWM34"}},{"cell_type":"markdown","source":["### Exercise 2: Delta Lake Operations - Read, Write, Update, Delete, Merge"],"metadata":{"id":"gpUhk9pfSHHF"}},{"cell_type":"markdown","source":["1: Read Data from Delta Lake"],"metadata":{"id":"u93OkZwATkug"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"_cFkZUAyU0DN"}},{"cell_type":"code","source":["# Read the transactional data from the Delta table\n","transactions_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n","\n","# Display the first 5 rows\n","transactions_df.show(5)"],"metadata":{"id":"9bU-TvAPTpRa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sql"],"metadata":{"id":"c705atXIU1Uy"}},{"cell_type":"code","source":["SELECT *\n","FROM final_transactions\n","LIMIT 5;"],"metadata":{"id":"taXwP2tkU3ug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2: Write Data to Delta Lake"],"metadata":{"id":"aVR7UBDmUnuB"}},{"cell_type":"code","source":["# Define new transactions to append\n","new_transactions = [\n","    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n","    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n","]\n","\n","# Create a DataFrame for new transactions\n","new_transactions_df = spark.createDataFrame(new_transactions, [\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n","\n","# Append new transactions to the Delta table\n","new_transactions_df.write.format(\"delta\").mode(\"append\").save(\"/content/sample_data/delta/final_transactions\")\n"],"metadata":{"id":"J6K7m08XUl1H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3: Update Data in Delta Lake"],"metadata":{"id":"mNjcKbi8UrpA"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"oHP3dgG2U9S9"}},{"cell_type":"code","source":["from delta.tables import *\n","\n","# Load the Delta table\n","delta_table = DeltaTable.forPath(spark, \"/content/sample_data/delta/final_transactions\")\n","\n","# Update the Price of Product 'Laptop'\n","delta_table.update(\n","    condition=\"Product = 'Laptop'\",\n","    set={\"Price\": \"1300\"}\n",")\n","\n","# Verify the update\n","transactions_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n","transactions_df.filter(\"Product = 'Laptop'\").show()\n"],"metadata":{"id":"w0_YbbWPU--j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sql"],"metadata":{"id":"vNdKnNqXVBbx"}},{"cell_type":"code","source":["-- Update the Price of Product = 'Laptop'\n","UPDATE final_transactions\n","SET Price = 1300\n","WHERE Product = 'Laptop';\n","\n","-- Verify the update\n","SELECT *\n","FROM final_transactions\n","WHERE Product = 'Laptop';\n"],"metadata":{"id":"xH03i_apVCHr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4: Delete Data from Delta Lake"],"metadata":{"id":"nt7TrvdnVE_I"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"p2oEGUSQVH8C"}},{"cell_type":"code","source":["# Delete all transactions where Quantity is less than 3\n","delta_table.delete(\"Quantity < 3\")\n","\n","# Verify the deletion\n","transactions_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n","transactions_df.show()"],"metadata":{"id":"FTuMTKvtVJHq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sql"],"metadata":{"id":"-cfhlDiAVL7s"}},{"cell_type":"code","source":["-- Delete all transactions where Quantity is less than 3\n","DELETE FROM final_transactions\n","WHERE Quantity < 3;\n","\n","-- Verify the deletion\n","SELECT *\n","FROM final_transactions;\n"],"metadata":{"id":"Q-qjNVtMVMvr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 5: Merge Data into Delta Lake"],"metadata":{"id":"C9JQtVd_VO5f"}},{"cell_type":"code","source":["# Create new data for merging\n","merge_data = [\n","    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1250),  # Updated Price\n","    (8, \"2024-09-08\", \"C007\", \"Charger\", 2, 30)    # New Transaction\n","]\n","\n","# Create a DataFrame for the merge data\n","merge_df = spark.createDataFrame(merge_data, [\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n","\n","# Perform the merge operation\n","delta_table.alias(\"t\").merge(\n","    merge_df.alias(\"s\"),\n","    \"t.TransactionID = s.TransactionID\"\n",").whenMatchedUpdate(set={\n","    \"Price\": \"s.Price\",\n","    \"Quantity\": \"s.Quantity\",\n","    \"TransactionDate\": \"s.TransactionDate\",\n","    \"CustomerID\": \"s.CustomerID\",\n","    \"Product\": \"s.Product\"\n","}).whenNotMatchedInsert(values={\n","    \"TransactionID\": \"s.TransactionID\",\n","    \"TransactionDate\": \"s.TransactionDate\",\n","    \"CustomerID\": \"s.CustomerID\",\n","    \"Product\": \"s.Product\",\n","    \"Quantity\": \"s.Quantity\",\n","    \"Price\": \"s.Price\"\n","}).execute()\n"],"metadata":{"id":"k5-XiVxjVUtm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 3: Delta Lake - History, Time Travel, and Vacuum"],"metadata":{"id":"2WK_h63JWznn"}},{"cell_type":"markdown","source":["1. View Delta Table History"],"metadata":{"id":"nD4NJju1W1WJ"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"YJldKO0CW4ge"}},{"cell_type":"code","source":["history_df = spark.sql(\"DESCRIBE HISTORY delta.`/content/sample_data/delta/final_transactions`\")\n","history_df.show(truncate=False)"],"metadata":{"id":"2cHchHyrW5jn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sql"],"metadata":{"id":"yL6-mk78W83Q"}},{"cell_type":"code","source":["DESCRIBE HISTORY delta.`/content/sample_data/delta/final_transactions`;"],"metadata":{"id":"mrAlyF4aW-L4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Perform Time Travel"],"metadata":{"id":"DPmqoDNFXLXj"}},{"cell_type":"markdown","source":["Retrieve State 5 Versions Ago (Using PySpark):"],"metadata":{"id":"lB7ySB3TXPDB"}},{"cell_type":"code","source":["time_travel_df = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"/content/sample_data/delta/final_transactions\")\n","time_travel_df.show()"],"metadata":{"id":"eEh9M2jXXSIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Retrieve Data by Timestamp (Using SQL):"],"metadata":{"id":"hIdgKICOXUzK"}},{"cell_type":"code","source":["SELECT *\n","FROM delta.`/content/sample_data/delta/final_transactions`\n","WHERE TransactionDate < '2024-09-04'\n","ORDER BY TransactionDate;"],"metadata":{"id":"ex9sP3I-XXMY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Vacuum the Delta Table"],"metadata":{"id":"QQITTjQvXfJN"}},{"cell_type":"markdown","source":["python"],"metadata":{"id":"lmKLB-wqXiNa"}},{"cell_type":"code","source":["spark.sql(\"VACUUM delta.`/path/to/delta/table` RETAIN 168 HOURS\")\n","\n","# Check the current state of the table\n","current_state_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n","current_state_df.show()"],"metadata":{"id":"3yhZ9YpTXm1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Converting Parquet Files to Delta Files"],"metadata":{"id":"f27HtPMCXqXa"}},{"cell_type":"markdown","source":["Step 1: Create a Parquet Table from Raw Transactions CSV:"],"metadata":{"id":"GTT0WrqSXumi"}},{"cell_type":"code","source":["csv_df = spark.read.csv(\"/content/sample_data/final_transactions\", header=True, inferSchema=True)\n","csv_df.write.parquet(\"/content/sample_data/parquet/table\")"],"metadata":{"id":"8p-Cd-BpXxYs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 2: Convert Parquet Table to Delta Table:"],"metadata":{"id":"mMuwITlSX0IE"}},{"cell_type":"code","source":["parquet_df = spark.read.parquet(\"/content/sample_data/parquet/table\")\n","parquet_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/converted_table\")\n"],"metadata":{"id":"Z0fQbuSkX2zL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 4: Implementing Incremental Load Pattern using Delta Lake"],"metadata":{"id":"P24AzvctYQzb"}},{"cell_type":"markdown","source":["1. Set Up Initial Data"],"metadata":{"id":"lzFO8ko9YVWX"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n","\n","spark = SparkSession.builder.appName(\"IncrementalLoadExample\").getOrCreate()\n","\n","# Define schema for transactions\n","schema = StructType([\n","    StructField(\"TransactionID\", IntegerType(), True),\n","    StructField(\"TransactionDate\", DateType(), True),\n","    StructField(\"CustomerID\", StringType(), True),\n","    StructField(\"Product\", StringType(), True),\n","    StructField(\"Quantity\", IntegerType(), True),\n","    StructField(\"Price\", IntegerType(), True)\n","])\n","\n","# Initial data (first three days)\n","initial_data = [\n","    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1200),\n","    (2, \"2024-09-02\", \"C002\", \"Tablet\", 2, 300),\n","    (3, \"2024-09-03\", \"C001\", \"Headphones\", 5, 50)\n","]\n","\n","initial_df = spark.createDataFrame(initial_data, schema)\n","\n","# Write initial data to Delta table\n","initial_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/final_transactions\")\n"],"metadata":{"id":"5-P-zeS4YYQf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Set Up Incremental Data"],"metadata":{"id":"wXCtVx86YbLU"}},{"cell_type":"code","source":["incremental_data = [\n","    (4, \"2024-09-04\", \"C003\", \"Smartphone\", 1, 800),\n","    (5, \"2024-09-05\", \"C004\", \"Smartwatch\", 3, 200),\n","    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n","    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n","]\n","\n","incremental_df = spark.createDataFrame(incremental_data, schema)\n","\n","# Append the new transactions to the Delta table\n","incremental_df.write.format(\"delta\").mode(\"append\").save(\"/content/sample_data/delta/final_transactions\")\n"],"metadata":{"id":"FUW9qkiuYeDR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Implement Incremental Load"],"metadata":{"id":"ojcVrQAWYigy"}},{"cell_type":"code","source":["# Read new transactions only (after 2024-09-03)\n","latest_date = \"2024-09-03\"\n","new_transactions_df = incremental_df.filter(f\"TransactionDate > '{latest_date}'\")\n","\n","# Append new transactions to the Delta table\n","new_transactions_df.write.format(\"delta\").mode(\"append\").save(\"/content/sample_data/delta/final_transactions\")\n"],"metadata":{"id":"9Qapy6EGYmIb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Monitor Incremental Load"],"metadata":{"id":"-ChOEaz0Yoif"}},{"cell_type":"code","source":["# View the Delta table history\n","history_df = spark.sql(\"DESCRIBE HISTORY delta.`/content/sample_data/delta/final_transactions`\")\n","history_df.show(truncate=False)\n","\n","# Verify the contents of the Delta table\n","final_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n","final_df.show()\n"],"metadata":{"id":"qHm7u1EPYoHZ"},"execution_count":null,"outputs":[]}]}