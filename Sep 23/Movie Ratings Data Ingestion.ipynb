{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWDuAeEjb5WoUSAyHgyx1q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hc8RcIy3Kqp-","executionInfo":{"status":"ok","timestamp":1727066311819,"user_tz":-330,"elapsed":56866,"user":{"displayName":"VIDHYA LATCHOUMI T","userId":"15527498621438682456"}},"outputId":"635bd873-12cb-4cd9-f4e3-3a8b830c440e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=290a6bd6c565b54c6c9f72738e5c0a07f6feda3901893d9c082e5747111499e2\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.2\n"]}],"source":["! pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import input_file_name\n","from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n","import os\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"MovieRatingsIngestion\").getOrCreate()\n","\n","# Define the schema for the movie ratings data\n","schema = StructType([\n","    StructField(\"UserID\", StringType(), True),\n","    StructField(\"MovieID\", StringType(), True),\n","    StructField(\"Rating\", FloatType(), True),\n","    StructField(\"Timestamp\", TimestampType(), True)\n","])\n","\n","# Define paths\n","raw_data_path = \"/content/sample_data/movie_ratings.csv\"\n","delta_table_path = \"/content/sample_data/delta/movie_ratings\"\n","\n","# Check if the raw data file exists\n","if os.path.exists(raw_data_path):\n","    try:\n","        # Read the CSV file into a DataFrame\n","        ratings_df = spark.read.csv(raw_data_path, schema=schema, header=True).withColumn(\"file_name\", input_file_name())\n","\n","        # Write the DataFrame to a Delta table\n","        ratings_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n","        print(\"Data loaded and saved as Delta table.\")\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","else:\n","    print(f\"File not found: {raw_data_path}\")\n"],"metadata":{"id":"D4jncjqwLVDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 2: Data Cleaning"],"metadata":{"id":"YLRXoAcxLWJk"}},{"cell_type":"code","source":["# Read the raw Delta table\n","raw_ratings_df = spark.read.format(\"delta\").load(delta_table_path)\n","\n","# Clean the DataFrame\n","cleaned_ratings_df = raw_ratings_df.filter((raw_ratings_df.Rating >= 1) & (raw_ratings_df.Rating <= 5))\n","\n","# Remove duplicates based on UserID and MovieID\n","cleaned_ratings_df = cleaned_ratings_df.dropDuplicates([\"UserID\", \"MovieID\"])\n","\n","# Save the cleaned data to a new Delta table\n","cleaned_delta_path = \"/content/sample_data/delta/cleaned_movie_ratings\"\n","cleaned_ratings_df.write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_path)\n","print(\"Cleaned data saved to Delta table.\")\n"],"metadata":{"id":"RnHf4wK9LYmF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 3: Movie Rating Analysis"],"metadata":{"id":"k4l0JJUaLb2G"}},{"cell_type":"code","source":["# Read the cleaned data\n","cleaned_ratings_df = spark.read.format(\"delta\").load(cleaned_delta_path)\n","\n","# Calculate average rating for each movie\n","average_ratings = cleaned_ratings_df.groupBy(\"MovieID\").agg({\"Rating\": \"avg\"}).withColumnRenamed(\"avg(Rating)\", \"AverageRating\")\n","average_ratings.show()\n","\n","# Identify movies with the highest and lowest average ratings\n","highest_rating = average_ratings.orderBy(\"AverageRating\", ascending=False).limit(1)\n","lowest_rating = average_ratings.orderBy(\"AverageRating\", ascending=True).limit(1)\n","\n","highest_rating.show()\n","lowest_rating.show()\n","\n","# Save analysis results to a Delta table\n","average_ratings.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/average_movie_ratings\")\n"],"metadata":{"id":"ByK1nT-7Lfx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 4: Time Travel and Delta Lake History"],"metadata":{"id":"5X72sL8BLimZ"}},{"cell_type":"code","source":["# Perform an update to the movie ratings data\n","updated_ratings_df = cleaned_ratings_df.withColumn(\"Rating\",\n","    when(cleaned_ratings_df.UserID == \"U001\", 5).otherwise(cleaned_ratings_df.Rating))\n","updated_ratings_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n","\n","# Roll back to the previous version of the Delta table\n","spark.sql(\"RESTORE TABLE delta.`/content/sample_data/delta/movie_ratings` TO VERSION AS OF 0\")\n","\n","# Use DESCRIBE HISTORY to view the history of changes\n","history_df = spark.sql(\"DESCRIBE HISTORY delta.`/content/sample_data/delta/movie_ratings`\")\n","history_df.show()\n"],"metadata":{"id":"i6cGtMNpLlJg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 5: Optimize Delta Table"],"metadata":{"id":"S2Z58AArLo64"}},{"cell_type":"code","source":["# Implement Z-ordering on the MovieID column to improve query performance\n","spark.sql(\"OPTIMIZE delta.`/content/sample_data/delta/movie_ratings` ZORDER BY (MovieID)\")\n","\n","# Compact the data and improve performance\n","spark.sql(\"OPTIMIZE delta.`/content/sample_data/delta/movie_ratings`\")\n","\n","# Use VACUUM to clean up older versions of the table\n","spark.sql(\"VACUUM delta.`/content/sample_data/delta/movie_ratings` RETAIN 168 HOURS\")\n"],"metadata":{"id":"NwavUY6yLrRX"},"execution_count":null,"outputs":[]}]}