{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut9QQScq-9fY",
        "outputId": "b084c5f3-fb33-4863-e375-9605f8913892"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=d7a8a2180b5b314a67c11ad204850fec7c2c8a9007a4e66b36a39a46027852e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
        "from pyspark.sql.functions import col, unix_timestamp, lag, avg, count, month, dayofmonth, hour, when\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
        "\n",
        "# Define the schema for user data\n",
        "user_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"user_name\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"age_group\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Define the schema for transaction data\n",
        "transaction_schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"transaction_amount\", FloatType(), True),\n",
        "    StructField(\"transaction_time\", TimestampType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"fraud_label\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load user data from CSV\n",
        "user_dim = spark.read.csv(\"/content/sample_data/user_dim.csv\", schema=user_schema, header=True)\n",
        "\n",
        "# Load transaction data from CSV\n",
        "transaction_fact = spark.read.csv(\"/content/sample_data/transaction_data.csv\", schema=transaction_schema, header=True)\n",
        "\n",
        "# Data Cleaning: Remove duplicates and null values\n",
        "user_dim = user_dim.dropDuplicates().na.drop()\n",
        "transaction_fact = transaction_fact.dropDuplicates().na.drop()\n",
        "\n",
        "# Data Cleaning: Remove negative transaction amounts\n",
        "transaction_fact = transaction_fact.filter(col(\"transaction_amount\") >= 0)\n",
        "\n",
        "# Feature Engineering\n",
        "# 1. Transaction Day, Month, and Hour\n",
        "transaction_fact = transaction_fact.withColumn(\"transaction_day\", dayofmonth(col(\"transaction_time\"))) \\\n",
        "                                     .withColumn(\"transaction_month\", month(col(\"transaction_time\"))) \\\n",
        "                                     .withColumn(\"transaction_hour\", hour(col(\"transaction_time\")))\n",
        "\n",
        "# 2. Total Transactions per User\n",
        "total_transactions = transaction_fact.groupBy(\"user_id\").agg(count(\"transaction_id\").alias(\"total_transactions\"))\n",
        "transaction_fact = transaction_fact.join(total_transactions, \"user_id\", \"left\")\n",
        "\n",
        "# 3. Average Transaction Amount per User\n",
        "average_transaction = transaction_fact.groupBy(\"user_id\").agg(avg(\"transaction_amount\").alias(\"avg_transaction_amount\"))\n",
        "transaction_fact = transaction_fact.join(average_transaction, \"user_id\", \"left\")\n",
        "\n",
        "# 4. Flagging Suspicious Transactions\n",
        "transaction_fact = transaction_fact.withColumn(\"is_suspicious\", when(\n",
        "    (col(\"transaction_amount\") > 10000) |\n",
        "    (col(\"total_transactions\") > 5), 1).otherwise(0))\n",
        "\n",
        "# Show anomalies\n",
        "transaction_fact.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueP7anrfAMPE",
        "outputId": "2bce392c-0a54-456e-9f0f-51fa3f3e2c60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+------------------+-------------------+-----------+-----------+---------------+-----------------+----------------+------------------+----------------------+-------------+\n",
            "|user_id|transaction_id|transaction_amount|transaction_time   |location   |fraud_label|transaction_day|transaction_month|transaction_hour|total_transactions|avg_transaction_amount|is_suspicious|\n",
            "+-------+--------------+------------------+-------------------+-----------+-----------+---------------+-----------------+----------------+------------------+----------------------+-------------+\n",
            "|2      |103           |15000.0           |2024-08-28 11:00:00|Los Angeles|1          |28             |8                |11              |1                 |15000.0               |1            |\n",
            "|1      |101           |5000.0            |2024-08-28 10:15:00|New York   |0          |28             |8                |10              |2                 |8500.0                |0            |\n",
            "|4      |105           |7500.0            |2024-08-28 13:45:00|Houston    |0          |28             |8                |13              |3                 |19166.666666666668    |0            |\n",
            "|5      |108           |4000.0            |2024-08-29 13:55:00|New York   |0          |29             |8                |13              |3                 |6666.666666666667     |0            |\n",
            "|4      |107           |25000.0           |2024-08-29 12:45:00|Houston    |1          |29             |8                |12              |3                 |19166.666666666668    |1            |\n",
            "|4      |109           |25000.0           |2024-08-29 12:45:00|Houston    |1          |29             |8                |12              |3                 |19166.666666666668    |1            |\n",
            "|5      |106           |12000.0           |2024-08-28 14:10:00|Phoenix    |1          |28             |8                |14              |3                 |6666.666666666667     |1            |\n",
            "|3      |104           |2000.0            |2024-08-28 12:30:00|New York   |0          |28             |8                |12              |1                 |2000.0                |0            |\n",
            "|5      |110           |4000.0            |2024-08-29 13:55:00|Phoenix    |0          |29             |8                |13              |3                 |6666.666666666667     |0            |\n",
            "|1      |102           |12000.0           |2024-08-28 10:30:00|New York   |0          |28             |8                |10              |2                 |8500.0                |1            |\n",
            "+-------+--------------+------------------+-------------------+-----------+-----------+---------------+-----------------+----------------+------------------+----------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}