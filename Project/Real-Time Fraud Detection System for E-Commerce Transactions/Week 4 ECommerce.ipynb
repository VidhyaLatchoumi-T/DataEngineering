{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWXOXukLr8Yo",
        "outputId": "bbccd795-7fba-42c2-ae85-6adca312d8be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=c688e48f42c718fc3c436b277fa4d1a4dc839d469579e25447a3d2c2b34f5bb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c6c2389d-d691-4c07-9efe-a9f04fb74a0b",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2oiIZer4M5",
        "outputId": "c3e3e692-9b2e-45de-dce2-af8b8eaad50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- transaction_id: integer (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- location_diff: double (nullable = true)\n",
            " |-- fraud_label: integer (nullable = true)\n",
            " |-- transaction_time: timestamp (nullable = true)\n",
            "\n",
            "+--------------+-------+------+-------------+-----------+-------------------+\n",
            "|transaction_id|user_id|amount|location_diff|fraud_label|   transaction_time|\n",
            "+--------------+-------+------+-------------+-----------+-------------------+\n",
            "|             1|    100| 250.0|          0.5|          0|2024-09-01 10:00:00|\n",
            "|             2|    101| 500.0|          1.0|          0|2024-09-01 11:30:00|\n",
            "|             3|    102|1000.0|          0.3|          1|2024-09-01 12:00:00|\n",
            "|             4|    100| 150.0|          0.6|          0|2024-09-01 13:00:00|\n",
            "|             5|    101| 750.0|          1.2|          1|2024-09-01 14:00:00|\n",
            "|             6|    102| 200.0|          0.8|          0|2024-09-01 15:00:00|\n",
            "|             7|    100| 300.0|          1.1|          0|2024-09-01 16:00:00|\n",
            "|             8|    101| 850.0|          0.4|          1|2024-09-01 17:00:00|\n",
            "|             9|    102|1200.0|          0.9|          1|2024-09-01 18:00:00|\n",
            "|            10|    100| 100.0|          0.2|          0|2024-09-01 19:00:00|\n",
            "|            11|    103| 250.0|          0.5|          0|2024-09-01 20:00:00|\n",
            "|            12|    104| 600.0|          1.0|          1|2024-09-01 21:30:00|\n",
            "|            13|    105| 750.0|          0.3|          0|2024-09-01 22:00:00|\n",
            "|            14|    103| 350.0|          0.6|          0|2024-09-01 23:00:00|\n",
            "|            15|    104| 950.0|          1.2|          1|2024-09-01 23:30:00|\n",
            "|            16|    105| 100.0|          0.8|          0|2024-09-02 00:00:00|\n",
            "|            17|    106| 400.0|          0.1|          0|2024-09-02 01:00:00|\n",
            "|            18|    107| 850.0|          0.4|          1|2024-09-02 02:00:00|\n",
            "|            19|    108|1100.0|          0.9|          0|2024-09-02 03:00:00|\n",
            "|            20|    106| 200.0|          0.6|          0|2024-09-02 04:00:00|\n",
            "+--------------+-------+------+-------------+-----------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------+-------+------+-------------+-----------+----------+\n",
            "|transaction_id|user_id|amount|location_diff|fraud_label|prediction|\n",
            "+--------------+-------+------+-------------+-----------+----------+\n",
            "|1             |100    |250.0 |0.5          |0          |0.0       |\n",
            "|2             |101    |500.0 |1.0          |0          |0.0       |\n",
            "|3             |102    |1000.0|0.3          |1          |1.0       |\n",
            "|4             |100    |150.0 |0.6          |0          |0.0       |\n",
            "|5             |101    |750.0 |1.2          |1          |1.0       |\n",
            "|6             |102    |200.0 |0.8          |0          |0.0       |\n",
            "|7             |100    |300.0 |1.1          |0          |0.0       |\n",
            "|8             |101    |850.0 |0.4          |1          |1.0       |\n",
            "|9             |102    |1200.0|0.9          |1          |1.0       |\n",
            "|10            |100    |100.0 |0.2          |0          |0.0       |\n",
            "|11            |103    |250.0 |0.5          |0          |0.0       |\n",
            "|12            |104    |600.0 |1.0          |1          |1.0       |\n",
            "|13            |105    |750.0 |0.3          |0          |0.0       |\n",
            "|14            |103    |350.0 |0.6          |0          |0.0       |\n",
            "|15            |104    |950.0 |1.2          |1          |1.0       |\n",
            "|16            |105    |100.0 |0.8          |0          |0.0       |\n",
            "|17            |106    |400.0 |0.1          |0          |0.0       |\n",
            "|20            |106    |200.0 |0.6          |0          |0.0       |\n",
            "+--------------+-------+------+-------------+-----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col, avg, stddev, hour\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
        "\n",
        "# Initialize Sparksession\n",
        "spark = SparkSession.builder.appName(\"FraudDetectionModelECommerce\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"amount\", DoubleType(), True),\n",
        "    StructField(\"location_diff\", DoubleType(), True),\n",
        "    StructField(\"fraud_label\", IntegerType(), True),\n",
        "    StructField(\"transaction_time\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Load historical transaction data\n",
        "transaction_df = spark.read.csv('/content/sample_data/ecom_transaction_data.csv', header=True, schema=schema)\n",
        "\n",
        "# Display schema and initial data for verification\n",
        "transaction_df.printSchema()\n",
        "transaction_df.show()\n",
        "\n",
        "# Feature Engineering\n",
        "# Extract time-based features\n",
        "transaction_df = transaction_df.withColumn(\"transaction_hour\", hour(col(\"transaction_time\")))\n",
        "\n",
        "# User-level statistics (Calculate average and stddev of transaction amount per user)\n",
        "user_stats = transaction_df.groupBy(\"user_id\").agg(\n",
        "    avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
        "    stddev(\"amount\").alias(\"stddev_transaction_amount\")\n",
        ")\n",
        "\n",
        "# Join user statistics\n",
        "transaction_df = transaction_df.join(user_stats, on=\"user_id\", how=\"left\")\n",
        "\n",
        "# Transaction amount deviation from user's average\n",
        "transaction_df = transaction_df.withColumn(\n",
        "    \"amount_deviation\",\n",
        "    (col(\"amount\") - col(\"avg_transaction_amount\")) / col(\"stddev_transaction_amount\")\n",
        ")\n",
        "\n",
        "# Assemble all the features into a single vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"amount\", \"transaction_hour\", \"location_diff\", \"amount_deviation\"],\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "# Apply assembler and transform the DataFrame\n",
        "transaction_df_assembled = assembler.transform(transaction_df)\n",
        "\n",
        "# Check if the DataFrame is empty or contains any null values in the features\n",
        "if transaction_df_assembled.rdd.isEmpty() or transaction_df_assembled.filter(col(\"features\").isNull()).count() > 0:\n",
        "    print(\"Error: No valid data available for model training.\")\n",
        "else:\n",
        "    # Train a logistic regression model to predict fraudulent transactions\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"fraud_label\")\n",
        "    model = lr.fit(transaction_df_assembled)\n",
        "\n",
        "    # Output predictions\n",
        "    predictions = model.transform(transaction_df_assembled)\n",
        "\n",
        "    # Show predictions\n",
        "    predictions.select(\"transaction_id\", \"user_id\", \"amount\", \"location_diff\", \"fraud_label\", \"prediction\").show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Week 4 ECommerce",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}