{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "213b3482-f153-44da-8933-1ed0049ec9e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Create Delta Tables Using 3 Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e426ca-eed3-4b0e-843c-06862bccaef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n",
      "|   1002|2024-01-16|      C002|Widget B|       5|15.75|\n",
      "|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n",
      "|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n",
      "|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n",
      "|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n",
      "|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n",
      "|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. Load the sales_data.csv file into a DataFrame.\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/sales_data.csv\", \"dbfs:/FileStore/sales_data.csv\")\n",
    "df_sales=spark.read.format(\"csv\").option (\"header\", \"true\").load(\"/FileStore/sales_data.csv\")\n",
    "df_sales.show()\n",
    "\n",
    "#2. Write the DataFrame as a Delta Table.\n",
    "df_sales.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sales_data\")\n",
    "df_sales=spark.read.format(\"delta\").load(\"/delta/sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba34caa5-633b-4908-8b25-0f46238e5e66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------+----------+\n",
      "|CustomerID| CustomerName|Region|SignupDate|\n",
      "+----------+-------------+------+----------+\n",
      "|      C001|     John Doe| North|2022-07-01|\n",
      "|      C002|   Jane Smith| South|2023-02-15|\n",
      "|      C003|Emily Johnson|  East|2021-11-20|\n",
      "|      C004|Michael Brown|  West|2022-12-05|\n",
      "|      C005|  Linda Davis| North|2023-03-10|\n",
      "+----------+-------------+------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Load the customer_data.json file into a DataFrame.\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "schema=StructType([  \n",
    "                   StructField(\"CustomerID\", StringType(), True),\n",
    "                   StructField(\"CustomerName\", StringType(), True),\n",
    "                   StructField(\"Region\", StringType(), True),\n",
    "                   StructField(\"SignupDate\", StringType(), True) \n",
    "])\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/customer_data.json\")\n",
    "\n",
    "df_customer=spark.read.format(\"json\").schema(schema).load(\"dbfs:/FileStore/customer_data.json\") \n",
    "df_customer.show()\n",
    "\n",
    "#4. Write the DataFrame as a Delta Table.\n",
    "df_customer.createOrReplaceTempView(\"customer_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta_customer\n",
    "USING DELTA\n",
    "AS SELECT * FROM customer_view\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b7c4b20-74c9-402b-b18c-dc7c59f31809",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d092902-303b-412d-bbb9-fbc8c01a63c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n",
      "|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n",
      "|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. Load the new_sales_data.csv file into a DataFrame.\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/new_sales_data.csv\", \"dbfs:/FileStore/new_sales_data.csv\")\n",
    "df_new_sales=spark.read.format(\"csv\").option (\"header\", \"true\").load(\"/FileStore/new_sales_data.csv\")\n",
    "df_new_sales.show()\n",
    "\n",
    "#2. Write the new DataFrame as a Delta Table.\n",
    "df_new_sales.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/new_sales_data\")\n",
    "df_new_sales=spark.read.format(\"delta\").load(\"/delta/new_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69074996-922e-4286-9572-a0bb8f1dfaf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales.createOrReplaceTempView(\"delta_sales\") \n",
    "df_new_sales.createOrReplaceTempView(\"new_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db4560f-1fb8-4319-aeca-79ed7078f3a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n",
      "|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n",
      "|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n",
      "|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n",
      "|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n",
      "|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n",
      "|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n",
      "|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n",
      "|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n",
      "|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3. Perform a MERGE INTO operation to update and insert records into the existing Delta table.\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_sales AS target \n",
    "          USING new_sales AS source \n",
    "          ON target.OrderID = source.OrderID \n",
    "          WHEN MATCHED THEN UPDATE SET target.OrderDate = source.OrderDate,target.CustomerID = source.CustomerID,target.Product = source.Product, target.Quantity = source.Quantity, target.Price = source.Price\n",
    "          WHEN NOT MATCHED THEN INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price)\n",
    "          VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT * FROM delta_sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6548c333-e7db-4941-9bbb-560b1183ac86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Optimize Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b80c8f-3dc2-45c6-9174-3b04ab53fc4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_sales_table USING DELTA LOCATION '/delta/sales_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366ea500-ed2d-4f13-8106-3ddc559ed910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Apply the OPTIMIZE command on the Delta Table and use Z-Ordering on an appropriate column.\n",
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_customer ZORDER BY CustomerID\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_sales_table ZORDER BY Product\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d765e58-8e91-4701-b668-1378510224ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e922090-4452-41e5-a883-c6debc48ea72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+------------------------------------------+\n",
      "|version|timestamp          |userId          |userName                          |operation             |operationParameters                                                                                                                                     |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                                |\n",
      "+-------+-------------------+----------------+----------------------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+------------------------------------------+\n",
      "|0      |2024-09-13 06:37:34|2929584751483774|azuser2141_mml.local@techademy.com|CREATE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{3532935135456107}|0912-053226-p5usobai|NULL       |WriteSerializable|true         |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1253}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "+-------+-------------------+----------------+----------------------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+------------------------------------------+\n",
      "\n",
      "+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                            |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |userMetadata|engineInfo                                |\n",
      "+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "|16     |2024-09-13 07:19:45|2929584751483774|azuser2141_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                                 |NULL|{3532935135456107}|0912-053226-p5usobai|15         |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 3216, p25FileSize -> 1794, numDeletionVectorsRemoved -> 1, minFileSize -> 1794, numAddedFiles -> 1, maxFileSize -> 1794, p75FileSize -> 1794, p50FileSize -> 1794, numAddedBytes -> 1794}                                                                                                                                                                                                                                                                                                                                                                                                                                                  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|15     |2024-09-13 07:19:42|2929584751483774|azuser2141_mml.local@techademy.com|MERGE    |{predicate -> [\"(OrderID#18875 = OrderID#19317)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{3532935135456107}|0912-053226-p5usobai|14         |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1495, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1886, materializeSourceTimeMs -> 141, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 557, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1141} |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|14     |2024-09-13 07:19:21|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|13         |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|13     |2024-09-13 07:19:09|2929584751483774|azuser2141_mml.local@techademy.com|MERGE    |{predicate -> [\"(OrderID#14248 = OrderID#14783)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{3532935135456107}|0912-053226-p5usobai|12         |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1494, numTargetBytesRemoved -> 1495, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 1082, materializeSourceTimeMs -> 89, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 379, numTargetRowsUpdated -> 3, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 562}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|12     |2024-09-13 07:18:10|2929584751483774|azuser2141_mml.local@techademy.com|MERGE    |{predicate -> [\"(OrderID#14248 = OrderID#14783)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{3532935135456107}|0912-053226-p5usobai|11         |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1495, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1735, materializeSourceTimeMs -> 117, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 661, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 910}  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|11     |2024-09-13 07:17:20|2929584751483774|azuser2141_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                                 |NULL|{3532935135456107}|0912-053226-p5usobai|10         |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 3200, p25FileSize -> 1787, numDeletionVectorsRemoved -> 1, minFileSize -> 1787, numAddedFiles -> 1, maxFileSize -> 1787, p75FileSize -> 1787, p50FileSize -> 1787, numAddedBytes -> 1787}                                                                                                                                                                                                                                                                                                                                                                                                                                                  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|10     |2024-09-13 07:17:19|2929584751483774|azuser2141_mml.local@techademy.com|MERGE    |{predicate -> [\"(OrderID#14248 = OrderID#14783)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{3532935135456107}|0912-053226-p5usobai|9          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1479, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1588, materializeSourceTimeMs -> 143, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 566, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 838}  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|9      |2024-09-13 07:14:54|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|8          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|8      |2024-09-13 07:12:28|2929584751483774|azuser2141_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                                 |NULL|{3532935135456107}|0912-053226-p5usobai|7          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 3200, p25FileSize -> 1787, numDeletionVectorsRemoved -> 1, minFileSize -> 1787, numAddedFiles -> 1, maxFileSize -> 1787, p75FileSize -> 1787, p50FileSize -> 1787, numAddedBytes -> 1787}                                                                                                                                                                                                                                                                                                                                                                                                                                                  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|7      |2024-09-13 07:12:26|2929584751483774|azuser2141_mml.local@techademy.com|MERGE    |{predicate -> [\"(OrderID#11970 = OrderID#12380)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{3532935135456107}|0912-053226-p5usobai|6          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1479, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1718, materializeSourceTimeMs -> 143, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 661, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 894}  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|6      |2024-09-13 07:10:53|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|5          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|5      |2024-09-13 07:10:06|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|4          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|4      |2024-09-13 06:36:01|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|3          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|3      |2024-09-13 06:33:20|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|2          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|2      |2024-09-13 06:33:03|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|1          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|1      |2024-09-13 06:25:03|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|0          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|0      |2024-09-13 06:23:44|2929584751483774|azuser2141_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{3532935135456107}|0912-053226-p5usobai|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Use DESCRIBE HISTORY to inspect the history of changes for a Delta Table.\n",
    "spark.sql(\"DESCRIBE HISTORY delta_customer\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"DESCRIBE HISTORY delta_sales_table\").show(truncate=False)\n",
    "\n",
    "#2. Use VACUUM to remove old files from the Delta Table.\n",
    "spark.sql(\"\"\"\n",
    "\tVACUUM delta_customer RETAIN 168 HOURS\n",
    "\"\"\") \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tVACUUM delta_sales_table RETAIN 168 HOURS\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8b9fc1b-789e-4dd9-a6bf-41f9c8cf0754",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "5. Hands-on Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6dc223-ef4d-425d-a923-0dd8cc6af428",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n",
      "|   1002|2024-01-16|      C002|Widget B|       5|15.75|\n",
      "|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n",
      "|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n",
      "|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n",
      "|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n",
      "|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n",
      "|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n",
      "|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n",
      "|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n",
      "|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n",
      "|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n",
      "|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n",
      "|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n",
      "|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n",
      "|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n",
      "|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Using Delta Lake for Data Versioning:\n",
    "df_version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"delta_sales_table\")\n",
    "df_version.show()\n",
    "\n",
    "#2. Building a Reliable Data Lake with Delta Lake:\n",
    "#Implement schema enforcement and handle data updates with Delta Lake.\n",
    "#schema enforcement\n",
    "from pyspark.sql.types import StructType, StructField,IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), nullable=False),\n",
    "    StructField(\"OrderDate\", StringType(), nullable=False),\n",
    "    StructField(\"CustomerID\", StringType(), nullable=False),\n",
    "    StructField(\"Product\", StringType(), nullable=False),\n",
    "    StructField(\"Quantity\", IntegerType(), nullable=False),\n",
    "    StructField(\"Price\", DoubleType(), nullable=False)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"dbfs:/FileStore/sales_data.csv\", schema=schema, header=True)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/delta_sales_table\")\n",
    "\n",
    "#handle data updates\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_sales AS target \n",
    "          USING new_sales AS source \n",
    "          ON target.OrderID = source.OrderID \n",
    "          WHEN MATCHED THEN UPDATE SET target.OrderDate = source.OrderDate,target.CustomerID = source.CustomerID,target.Product = source.Product, target.Quantity = source.Quantity, target.Price = source.Price\n",
    "          WHEN NOT MATCHED THEN INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price)\n",
    "          VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT * FROM delta_sales\").show()\n",
    "\n",
    "\n",
    "#Optimize data layout and perform vacuum operations to maintain storage efficiency\n",
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_customer ZORDER BY CustomerID\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "\tVACUUM delta_customer RETAIN 168 HOURS\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sep 13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
